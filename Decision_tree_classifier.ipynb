{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01992eaa-2eb6-4143-ad8c-d85c486381d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c28162-d2ea-4f3c-954f-eed974ce625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction indexing for classes\n",
    "def select_class_index(labels, n):\n",
    "    labels = np.array(labels)\n",
    "    selected = []\n",
    "\n",
    "    for c in range(10):\n",
    "        idx = np.where(labels == c)[0][:n]\n",
    "        selected.extend(idx)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208745e1-9d21-40a8-b2d5-13e1134ea7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n",
      "5000\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "# load in cifar-10 dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root=\"F:/CIFAR10_Project/data\", train=True, download=True)\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"F:/CIFAR10_Project/data\", train=False, download=True)\n",
    "\n",
    "# select the first 500 train images per class\n",
    "train_indices = select_class_index(train_set.targets, 500)\n",
    "train_subset = torch.utils.data.Subset(train_set, train_indices)\n",
    "\n",
    "# select the first 100 test images\n",
    "test_indices = select_class_index(test_set.targets, 100)\n",
    "test_subset = torch.utils.data.Subset(test_set, test_indices)\n",
    "\n",
    "print(len(train_subset))  # Should be 500 * 10 = 5000\n",
    "print(len(test_subset))   # Should be 1000\n",
    "\n",
    "print(len(train_indices))  # 5000\n",
    "print(set([train_set.targets[i] for i in train_indices])) # check how many classes ( should be 0-9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec77e5f5-4826-477e-ab93-fa7411f07378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize to 224x224x3\n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a1dc6b-3ca8-4444-9655-1132aa3791e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataloaders to hold the resized images\n",
    "\n",
    "train_subset.dataset.transform = transform_resnet\n",
    "test_subset.dataset.transform = transform_resnet\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f151d17-22ce-40d4-a4c8-94a558c9fac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load RestNet-18 and remove the last layer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "resnet = resnet18(weights=weights)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61190eda-cc3f-46ec-833b-6d5c3ededf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet-18 feature extractor\n",
    "#def extract_features(dataset):\n",
    "   # loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "def extract_features(dataset, batch_size=64):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    feats = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, lbls in loader:\n",
    "            images = images.to(device)\n",
    "            f = resnet(images)\n",
    "            feats.append(f.cpu())\n",
    "            labels.append(lbls)\n",
    "\n",
    "    return torch.cat(feats), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebbf3a0-3b43-4ed1-b7c6-76e908e3d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_features, train_labels = extract_features(train_subset.dataset)\n",
    "#test_features, test_labels = extract_features(test_subset.dataset)\n",
    "train_features, train_labels = extract_features(train_subset)\n",
    "test_features, test_labels = extract_features(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361e12b1-a148-4354-ad29-ef6053f7cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 50) (1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# TypeError: can't convert cuda:0 device type tensor to numpy. \n",
    "# Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "\n",
    "train_features_np = train_features.cpu().numpy()\n",
    "test_features_np = test_features.cpu().numpy()\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "train_pca = pca.fit_transform(train_features_np)\n",
    "test_pca  = pca.transform(test_features_np)\n",
    "\n",
    "print(train_pca.shape, test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71bc5e89-e20c-4d84-b7dc-a186facc0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecisionTree:\n",
    "    def __init__(self, max_depth=50, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        prob = counts / counts.sum()\n",
    "        return 1 - np.sum(prob ** 2)\n",
    "\n",
    "    def split(self, X_column, threshold):\n",
    "        left_index = np.where(X_column <= threshold)[0]\n",
    "        right_index = np.where(X_column > threshold)[0]\n",
    "        return left_index, right_index\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        m, n = X.shape\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        parent_gini = self.gini(y)\n",
    "        best_gini = 1\n",
    "        best_index, best_thr = None, None\n",
    "\n",
    "        for feature_index in range(n):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for thr in thresholds:\n",
    "                left_index, right_index = self.split(X[:, feature_index], thr)\n",
    "                if len(left_index) == 0 or len(right_index) == 0:\n",
    "                    continue\n",
    "\n",
    "                gini_left = self.gini(y[left_index])\n",
    "                gini_right = self.gini(y[right_index])\n",
    "                weighted_gini = (len(left_index) * gini_left + len(right_index) * gini_right) / m\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_index = feature_index\n",
    "                    best_thr = thr\n",
    "\n",
    "        #print(f\"Best split: feature {best_index}, threshold {best_thr}, gini {best_gini}\")\n",
    "        if best_gini >= parent_gini:\n",
    "            return None, None\n",
    "\n",
    "        return best_index, best_thr\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        #num_samples_per_class = [np.sum(y == c) for c in np.unique(y)]\n",
    "        #predicted_class = np.argmax(num_samples_per_class)\n",
    "        unique_classes = np.unique(y)\n",
    "        counts = [np.sum(y == c) for c in unique_classes]\n",
    "        predicted_class = unique_classes[np.argmax(counts)]\n",
    "\n",
    "        node = {\n",
    "            'depth': depth,\n",
    "            'num_samples': len(y),\n",
    "            'predicted_class': predicted_class\n",
    "        }\n",
    "\n",
    "        if depth < self.max_depth and len(y) >= self.min_samples_split:\n",
    "            feature_index, threshold = self.best_split(X, y)\n",
    "            if feature_index is not None:\n",
    "                #print(f\"Depth {depth}: trying to split, samples: {len(y)}\")\n",
    "                left_index, right_index = self.split(X[:, feature_index], threshold)\n",
    "                node['feature_index'] = feature_index\n",
    "                node['threshold'] = threshold\n",
    "                node['left'] = self.build_tree(X[left_index], y[left_index], depth + 1)\n",
    "                node['right'] = self.build_tree(X[right_index], y[right_index], depth + 1)\n",
    "\n",
    "                \n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_sample(self, node, x):\n",
    "        if 'feature_index' not in node:\n",
    "            return node['predicted_class']\n",
    "\n",
    "        if x[node['feature_index']] <= node['threshold']:\n",
    "            return self.predict_sample(node['left'], x)\n",
    "        else:\n",
    "            return self.predict_sample(node['right'], x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_sample(self.tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0880b469-5426-43d0-99ec-dfabc0e4198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "tree = BasicDecisionTree(max_depth=50)\n",
    "tree.fit(train_pca, train_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea2a26c6-0353-40e3-8c1e-5b76f07ecae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training small depth model\n",
    "small_tree = BasicDecisionTree(max_depth=5)\n",
    "small_tree.fit(train_pca, train_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14302b37-629d-4602-bbe2-36f21fa9fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test samples\n",
    "tree_prediction = tree.predict(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21c7ed-1ed2-4fde-bb30-8abfd3b1e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_prediction = small_tree.predict(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a949b4f-ed82-4223-a462-957c0f9cdd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pca shape: (5000, 50)\n",
      "train_labels shape: torch.Size([5000])\n",
      "Unique train labels: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"train_pca shape:\", train_pca.shape)\n",
    "print(\"train_labels shape:\", train_labels.shape)\n",
    "print(\"Unique train labels:\", np.unique(train_labels.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25a212be-8cfb-4847-80f9-b522c1dea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit's decision tree\n",
    "\n",
    "dt_sklearn = DecisionTreeClassifier(max_depth=50, random_state=42)\n",
    "\n",
    "X_train = train_pca\n",
    "y_train = train_labels.numpy()\n",
    "X_test = test_pca\n",
    "y_test = test_labels.numpy()\n",
    "\n",
    "# Train using features \n",
    "dt_sklearn.fit(train_pca, y_train)\n",
    "\n",
    "# Predict on test features\n",
    "y_pred_sklearn = dt_sklearn.predict(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79c6309e-b765-4311-9dc4-dcf3c59b3840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('basic_decision_tree.pkl', 'wb') as f:\n",
    "    pickle.dump(tree, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b04f6ffc-6400-4869-8212-c53a57444471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the small tree model\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('depth5_decision_tree.pkl', 'wb') as f:\n",
    "    pickle.dump(small_tree, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ee89ba8-9b72-465d-8ebe-9f451bbd42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the model\n",
    "\n",
    "#with open('basic_decision_tree.pkl', 'rb') as f:\n",
    "    #loaded_tree = pickle.load(f)\n",
    "\n",
    "# Use loaded_tree to predict\n",
    "#y_pred = loaded_tree.predict(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075764f5-42c3-4b23-b8fe-9c80e10e50e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of 5-depth Decision Tree:\n",
      "on training set\n",
      "Accuracy: 0.6148\n",
      "Precision: 0.6238588991336521\n",
      "Recall : 0.6148\n",
      "F1-score : 0.6093440825420627\n",
      "Confusion Matrix:\n",
      " [[360  23   9   5   6   0   8  16  54  19]\n",
      " [ 10 404   1   1   1   0   5   4  12  62]\n",
      " [ 87   3 160  58  78  12  59  39   4   0]\n",
      " [ 24  16   2 264  13  70  84  13  13   1]\n",
      " [ 31   2  10  28 296  18  36  76   3   0]\n",
      " [ 10   4   9 182  13 217  21  39   5   0]\n",
      " [ 33   3  40  22  22   8 365   5   2   0]\n",
      " [ 16   6  11  74  48  22   7 296  20   0]\n",
      " [ 73  20   1   3   2   4  11  11 344  31]\n",
      " [  8  84   2   0   2   0   2   4  30 368]]\n",
      "on testing set\n",
      "Accuracy: 0.547\n",
      "Precision: 0.5635160327395776\n",
      "Recall : 0.547\n",
      "F1-score : 0.5392929132145003\n",
      "Confusion Matrix:\n",
      " [[66  6  1  0  0  1  4  3 15  4]\n",
      " [ 6 66  1  1  0  0  1  3  2 20]\n",
      " [21  0 22 18 12  4 19  4  0  0]\n",
      " [ 6  1  5 53  1 16 11  5  2  0]\n",
      " [ 5  0  1 12 49  4  9 19  1  0]\n",
      " [ 6  0  1 38  1 37  7  7  3  0]\n",
      " [ 8  0  3  3  2  4 79  0  1  0]\n",
      " [ 2  2  1 20 11 10  0 50  4  0]\n",
      " [22  2  1  1  0  0  1  4 57 12]\n",
      " [ 3 18  0  0  0  0  1  1  9 68]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of 5-depth Decision Tree:\")\n",
    "print(\"on training set\")\n",
    "train_prediction = small_tree.predict(train_pca) # replace by loaded_tree.predict(...) for the saved model\n",
    "evaluate_model(train_labels.numpy(), train_prediction)\n",
    "\n",
    "print(\"on testing set\")\n",
    "tree_prediction = small_tree.predict(test_pca)\n",
    "evaluate_model(test_labels.numpy(), tree_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33f6ff89-0df2-4618-b215-2e605fd01dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Basic Decision Tree:\n",
      "on training set\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall : 1.0\n",
      "F1-score : 1.0\n",
      "Confusion Matrix:\n",
      " [[500   0   0   0   0   0   0   0   0   0]\n",
      " [  0 500   0   0   0   0   0   0   0   0]\n",
      " [  0   0 500   0   0   0   0   0   0   0]\n",
      " [  0   0   0 500   0   0   0   0   0   0]\n",
      " [  0   0   0   0 500   0   0   0   0   0]\n",
      " [  0   0   0   0   0 500   0   0   0   0]\n",
      " [  0   0   0   0   0   0 500   0   0   0]\n",
      " [  0   0   0   0   0   0   0 500   0   0]\n",
      " [  0   0   0   0   0   0   0   0 500   0]\n",
      " [  0   0   0   0   0   0   0   0   0 500]]\n",
      "on testing set\n",
      "Accuracy: 0.599\n",
      "Precision: 0.5999590094660118\n",
      "Recall : 0.599\n",
      "F1-score : 0.5979631970770243\n",
      "Confusion Matrix:\n",
      " [[55  4 10  1  1  3  3  1 15  7]\n",
      " [ 4 76  2  1  0  0  0  0  3 14]\n",
      " [ 5  0 48 10  9  5 14  5  4  0]\n",
      " [ 1  1 13 42  6 21 11  2  1  2]\n",
      " [ 7  0 10  7 50  7  3 15  0  1]\n",
      " [ 0  0  8 15  7 61  4  3  1  1]\n",
      " [ 3  0  5  9  4  5 73  0  1  0]\n",
      " [ 1  0  2 10 17 15  0 53  1  1]\n",
      " [20  5  1  0  0  0  1  2 63  8]\n",
      " [ 4  8  0  2  0  0  0  0  8 78]]\n",
      "\n",
      "Evaluation of Scikit's Decision Tree:\n",
      "on training set\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall : 1.0\n",
      "F1-score : 1.0\n",
      "Confusion Matrix:\n",
      " [[500   0   0   0   0   0   0   0   0   0]\n",
      " [  0 500   0   0   0   0   0   0   0   0]\n",
      " [  0   0 500   0   0   0   0   0   0   0]\n",
      " [  0   0   0 500   0   0   0   0   0   0]\n",
      " [  0   0   0   0 500   0   0   0   0   0]\n",
      " [  0   0   0   0   0 500   0   0   0   0]\n",
      " [  0   0   0   0   0   0 500   0   0   0]\n",
      " [  0   0   0   0   0   0   0 500   0   0]\n",
      " [  0   0   0   0   0   0   0   0 500   0]\n",
      " [  0   0   0   0   0   0   0   0   0 500]]\n",
      "on testing set\n",
      "Accuracy: 0.589\n",
      "Precision: 0.5887726341295211\n",
      "Recall : 0.589\n",
      "F1-score : 0.5880550745559965\n",
      "Confusion Matrix:\n",
      " [[53  8  7  4  3  2  1  3 13  6]\n",
      " [ 4 76  1  1  1  0  0  0  4 13]\n",
      " [ 6  0 46 10 11  4 13  9  1  0]\n",
      " [ 2  2  9 42  6 23  8  5  1  2]\n",
      " [ 8  1  9  7 50  6  3 16  0  0]\n",
      " [ 0  0  6 15  6 61  5  5  1  1]\n",
      " [ 2  0  4  8  8  4 73  0  0  1]\n",
      " [ 1  0  4  7 14 14  0 59  1  0]\n",
      " [20  5  2  0  1  0  2  3 59  8]\n",
      " [ 4 12  0  5  0  0  0  2  7 70]]\n"
     ]
    }
   ],
   "source": [
    "# i've already made an evaluation script from scratch for Naive bayes.\n",
    "# I will now use the integrated evaluation functions from sklearn\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, average='macro'))\n",
    "    print(\"Recall :\", recall_score(y_true, y_pred, average='macro'))\n",
    "    print(\"F1-score :\", f1_score(y_true, y_pred, average='macro'))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"Evaluation of Basic Decision Tree:\")\n",
    "print(\"on training set\")\n",
    "train_prediction = tree.predict(train_pca) # replace by loaded_tree.predict(...) for the saved model\n",
    "evaluate_model(train_labels.numpy(), train_prediction)\n",
    "\n",
    "print(\"on testing set\")\n",
    "tree_prediction = tree.predict(test_pca)\n",
    "evaluate_model(test_labels.numpy(), tree_prediction)\n",
    "\n",
    "      \n",
    "print(\"\\nEvaluation of Scikit's Decision Tree:\")\n",
    "print(\"on training set\")\n",
    "y_pred_train_sklearn = dt_sklearn.predict(X_train)\n",
    "evaluate_model(y_train, y_pred_train_sklearn)\n",
    "print(\"on testing set\")\n",
    "evaluate_model(y_test, y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb8226-16a4-4817-b723-c5341d9d0658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
